{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import csv\n",
    "from csv import reader, writer, DictWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishing local directory path to Winter Turf SD Card folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/michaelfelzan/Desktop/DEMO_2021-2022_SD_Cards'"
      ]
     },
     "execution_count": 876,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enter local directory path to SD cards here:\n",
    "workindir = r'/Users/michaelfelzan/Desktop/DEMO_2021-2022_SD_Cards'\n",
    "os.chdir(workindir)\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (printing names of all SD card folders at base of directory):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winter_Turf_Type_B_-_6\n",
      "Winter_Turf_Type_B_-_1\n",
      "Winter_Turf_Type_A_-_7\n",
      "Winter_Turf_Type_A_-_8\n",
      "Winter_Turf_Type_A_-_10\n",
      "Winter_Turf_Type_A_-_28\n",
      "Winter_Turf_Type_A_-_17\n",
      "Winter_Turf_Type_A_-_21\n",
      "Winter_Turf_Type_A_-_19\n",
      "Winter_Turf_Type_A_-_26\n",
      "Winter_Turf_Type_A_-_18\n",
      "Winter_Turf_Type_A_-_27\n",
      "Winter_Turf_Type_A_-_20\n",
      "Winter_Turf_Type_A_-_16\n",
      "Winter_Turf_Type_A_-_34\n",
      "Winter_Turf_Type_B_-_12\n",
      "Winter_Turf_Type_A_-_35\n",
      "Winter_Turf_Type_B_-_5\n",
      "Winter_Turf_Type_A_-_3\n",
      "Winter_Turf_Type_A_-_2\n",
      "Winter_Turf_Type_A_-_5\n",
      "Winter_Turf_Type_B_-_4\n",
      "Winter_Turf_Type_A_-_14\n",
      "Winter_Turf_Type_A_-_22\n",
      "Winter_Turf_Type_A_-_23\n",
      "Winter_Turf_Type_A_-_24\n",
      "Winter_Turf_Type_A_-_12\n",
      "Winter_Turf_Type_A_-_15\n",
      "Winter_Turf_Type_B_-_11\n",
      "Winter_Turf_Type_A_-_31\n",
      "Winter_Turf_Type_A_-_36\n"
     ]
    }
   ],
   "source": [
    "node_folders = []\n",
    "\n",
    "for nodefoldername in os.listdir():\n",
    "    if nodefoldername != '.DS_Store':\n",
    "        node_folders.append(nodefoldername)\n",
    "        \n",
    "for folder in node_folders:\n",
    "    print(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining 'stand-alone' functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ListDirNoDSstore(path):\n",
    "    \"\"\"This function simply lists the contents of a\n",
    "    directory path, though leaves out the '.DS_Store' file.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    path: 'raw' str (path to directory)\n",
    "    \"\"\"\n",
    "    rawcontents = os.listdir(path)\n",
    "    contents = []\n",
    "    for item in rawcontents:\n",
    "        if item != '.DS_Store':\n",
    "            contents.append(item)\n",
    "    \n",
    "    return contents\n",
    "\n",
    "\n",
    "\n",
    "def LogInfoGetter(txtlog):\n",
    "    \"\"\"This function is utilized by some attributes/sub-functions\n",
    "    of the 'SubFolder()' class. The function parses the text file (log)\n",
    "    which is inputted, determines which rows are headers (column names)\n",
    "    vs. data, and returns a dict containing info about the .txt log.\n",
    "    \n",
    "    Info in the returned dict include:\n",
    "    'ColNames': a list, with all column names for the log in sequential order\n",
    "               (if the log has no column names, generic names Col_1, Col_2, etc.\n",
    "               are created for the log)\n",
    "    'Data':  The rows of the .txt which represent data (not headers); list\n",
    "    'AllLines': All rows of the data (headers and data) in list form\n",
    "    'HeaderIndicies': indicies which represent headers in the 'All Lines' list\n",
    "    'LogDate': date of the log (eg. 2021-09-17)\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    txtlog: 'raw' str (path to .txt file log)\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(txtlog, \"r\") as file_object:\n",
    "        Lines = file_object.readlines()\n",
    "    \n",
    "        cleanlines = [] # all lines, sans '\\n'\n",
    "        header_indicies = [] \n",
    "        column_names = []\n",
    "        row_entries = [] # data, corresponding to col. names\n",
    "    \n",
    "        for item in Lines:\n",
    "            cleanlines.append(item.split('\\n')[0])\n",
    "        \n",
    "        for line in cleanlines:\n",
    "            lineindex = cleanlines.index(line)\n",
    "            splitbycomma = line.split(',') #splitting each line in txt by commas\n",
    "        \n",
    "            # If a line in the .txt doesnt have a comma, it is classified\n",
    "            # as a header\n",
    "            try:\n",
    "                splitbycomma[1]\n",
    "            except:\n",
    "                header_indicies.append(lineindex)\n",
    "            \n",
    "            # If the first entry of a line can't be converted to a 'float'\n",
    "            # (column names all assummed to contain letter characters), then\n",
    "            # it is classified as a header.\n",
    "            try:\n",
    "                float(splitbycomma[0])\n",
    "            except:\n",
    "                if lineindex not in header_indicies:\n",
    "                    header_indicies.append(lineindex)\n",
    "\n",
    "        for line in cleanlines:\n",
    "            lineindex = cleanlines.index(line)   # getting index no.'s of all lines\n",
    "            if lineindex not in header_indicies: # if line is not a header:\n",
    "                linechunks = line.split(',')     # split by comma, and\n",
    "                row_entries.append(linechunks)   # append each chunk into 'row entries'\n",
    "        \n",
    "        try:\n",
    "            row_entries[0]\n",
    "        except:\n",
    "            return('error: no data in .txt') # this is returned if .txt is blank\n",
    "        \n",
    "        # If .txt actually has data:\n",
    "        else:\n",
    "            len_first_entry = len(row_entries[0]) # getting length of col's in first row,\n",
    "            for entry in row_entries:             # to confirm all other rows have the\n",
    "                if len_first_entry != len(entry): # same length as the first row.\n",
    "                    result = False\n",
    "                    print(\"All elements are not equal\")\n",
    "                    break\n",
    "    \n",
    "            # A Trigger variable for a 'detected header fit' is established here;\n",
    "            # the variable is effictively switched back to 'False' every time a \n",
    "            # new .txt file is opened.\n",
    "            header_fit_detected = False \n",
    "            for headerindex in header_indicies:\n",
    "                headeritem_w_commasplit = (((cleanlines[headerindex]).split(',')))\n",
    "                len_of_header = len(headeritem_w_commasplit)\n",
    "                # if the no. columns of header contender is same as len of first entry,\n",
    "                # AND the trigger isn't already set to 'True'....\n",
    "                # ...then the contents of header are assigned as the column names,\n",
    "                # and trigger is set to 'True' (errors out if there are 2 poss. matches)\n",
    "                if len_of_header == len_first_entry:\n",
    "                    if header_fit_detected == True:\n",
    "                        print(f\"ERROR -- two possible matches for headers detected for {txtlog}\")\n",
    "                        break\n",
    "                    else:\n",
    "                        header_fit_detected = True\n",
    "                        column_names = headeritem_w_commasplit\n",
    "            \n",
    "            # After all of that, if the header_fit_detected is still set to 'False',\n",
    "            # then .txt log is determined to have no header which represents col. names,\n",
    "            # and generic column names are created for the dataset\n",
    "            if header_fit_detected == False:\n",
    "                #print(\"no header detected for x; columns named numerically\")\n",
    "                for i in range(len_first_entry):\n",
    "                    column_names.append(f'Col_{i}') #Col_1, Col_2, Col_3, etc.\n",
    "\n",
    "            # (parsing .txt dir path for just the 'date' (eg. 2021-07-16))\n",
    "            firstnamesplit = txtlog.split('/Logs/')[1]\n",
    "            secondnamesplit = firstnamesplit.split('/Log')[1]\n",
    "            thirdnamesplit = secondnamesplit.split('.txt')[0]\n",
    "        \n",
    "            # Two new empty lists are created here, which will store the data collected\n",
    "            # by this function, PLUS the NODE_ID (eg. e00fce68816c2bc59976cdf2) and the\n",
    "            # log date (2000-01-01), so this may be stored on .CSV\n",
    "            data_w_nodeID_logdate_appended = []\n",
    "            colnames_w_nodeID_logdate_appended = []\n",
    "        \n",
    "            for datapiece in row_entries:\n",
    "                iterdatapiece = []\n",
    "                subfoldname_split1 = txtlog.split('/GEMS/')[1]\n",
    "                subfoldname = subfoldname_split1.split('/Logs/')[0]\n",
    "                iterdatapiece.append(subfoldname)    # appending node ID,\n",
    "                iterdatapiece.append(thirdnamesplit) # then log date,\n",
    "                for number in datapiece:             # then every data number \n",
    "                    iterdatapiece.append(number)     # within row \n",
    "                data_w_nodeID_logdate_appended.append(iterdatapiece)\n",
    "            \n",
    "            colnames_w_nodeID_logdate_appended.append('NODE_ID') # appending column names\n",
    "            colnames_w_nodeID_logdate_appended.append('log_date')# for this data as well.\n",
    "            for colname in column_names:\n",
    "                colnames_w_nodeID_logdate_appended.append(colname)\n",
    "    \n",
    "            return {\n",
    "                'ColNames': colnames_w_nodeID_logdate_appended,\n",
    "                'Data':  data_w_nodeID_logdate_appended,\n",
    "                'AllLines': cleanlines,\n",
    "                'HeaderIndicies': header_indicies,\n",
    "                'LogDate': thirdnamesplit\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeFolder:\n",
    "    \"\"\"This class gathers the properties of a 'Node Folder'\n",
    "    eg. 'Winter_Turf_Type_A_-_2', so that the various info\n",
    "    associated with one of these folders may be accessed\n",
    "    by calling the modules associated with the created \n",
    "    class object.\n",
    "    \n",
    "    Attributes\n",
    "    ------------\n",
    "    - nodename (str): Name of the 'Node' \n",
    "            eg. Winter_Turf_Type_A_-_2\n",
    "\n",
    "    Modules\n",
    "    ------------\n",
    "    - self.gems_folderpath : retrieves path to /GEMS/ dir\n",
    "    - self.node_subfolders : provides list containing \n",
    "            names of 'subfolders' associated with each\n",
    "            Node. (eg. e00fce68816c2bc59976cdf2)\n",
    "    - self.no_of_subfolders : lists number of subfolders\n",
    "            within each node folder (most have 1; though \n",
    "            some have 2.)\n",
    "    \"\"\"\n",
    "    def __init__(self, nodename):\n",
    "        self.nodename = nodename\n",
    "        \n",
    "        gemsfolderpath = os.path.join(workindir,nodename,'GEMS')\n",
    "        nodesubfolders = ListDirNoDSstore(gemsfolderpath)\n",
    "            \n",
    "        self.gems_folderpath = gemsfolderpath\n",
    "        self.node_subfolders = nodesubfolders\n",
    "        self.no_of_subfolders = len(nodesubfolders) \n",
    "        \n",
    "class SubNodeFolder:\n",
    "    \"\"\"This class gathers the properties of a 'Sub-Node \n",
    "    Folder' eg. 'e00fce68816c2bc59976cdf2', so that the \n",
    "    info associated with one of these subfolders may be \n",
    "    accessed by calling the modules associated with the \n",
    "    class object. \n",
    "    Additionally, this class includes two\n",
    "    functions -- one which gathers all log .txt paths \n",
    "    within a subfolder, and another which incorporates\n",
    "    the standalone 'LogInfoGetter()' function to create\n",
    "    a .csv file(s) associated with all .txt logs within\n",
    "    a subfolder.\n",
    "    \n",
    "    Attributes\n",
    "    ------------\n",
    "    - subfoldername (str) : eg. e00fce68816c2bc59976cdf2\n",
    "    - nodename (str) : eg. Winter_Turf_Type_A_-_2\n",
    "    - gemsfolderpath (raw str) eg. r'/Users/mf/Desktop/\n",
    "            SD_Cards/Winter_Turf_Type_A_-_23/GEMS'\n",
    "\n",
    "    Modules\n",
    "    ------------\n",
    "    - self.subfolder_path : path to subfolder on local dir\n",
    "    - self.subfolder_contents : list of items (folders \n",
    "            and files) within subfold\n",
    "    - self.path_to_logs : path to /Logs/ dir within subfold\n",
    "    - self.logs_contents : list of items within /Logs/\n",
    "    - self.years_as_ints : list of folders named by years\n",
    "        within /Logs/ eg. [2000, 2020, 2021]\n",
    "    - self.year_folders : year folders as strings\n",
    "    \n",
    "    Functions (descriptions for functions below)\n",
    "    ------------\n",
    "    - LogTextPathsRetriever(self)\n",
    "    - Log_CSV(self)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, subfoldername, nodename, gemsfolderpath):\n",
    "        self.subfoldername = subfoldername\n",
    "        self.nodename = nodename\n",
    "        self.gemsfolderpath = gemsfolderpath\n",
    "        \n",
    "        subfolderpath = os.path.join(gemsfolderpath, subfoldername)\n",
    "        subfoldercontents = ListDirNoDSstore(subfolderpath)\n",
    "        \n",
    "        path_to_logs = os.path.join(subfolderpath, 'Logs')\n",
    "        logs_contents = ListDirNoDSstore(path_to_logs)\n",
    "        \n",
    "        self.subfolder_path = subfolderpath  \n",
    "        self.subfolder_contents = subfoldercontents\n",
    "        self.path_to_logs = path_to_logs\n",
    "        self.logs_contents = logs_contents\n",
    "        \n",
    "        \n",
    "        non_year_items = ['UnsentLogs.dat', 'Header.dat', 'UnsentLogs.txt']\n",
    "        year_folders = []\n",
    "        years_as_ints = []\n",
    "        \n",
    "        for content in self.logs_contents:\n",
    "            if content not in non_year_items:\n",
    "                if '.csv' not in content:\n",
    "                    years_as_ints.append(int(content))\n",
    "                    \n",
    "        years_as_ints.sort()\n",
    "        \n",
    "        for intyear in years_as_ints:\n",
    "            year_folders.append(str(intyear))\n",
    "            \n",
    "        self.years_as_ints = years_as_ints\n",
    "        self.year_folders = year_folders\n",
    " \n",
    "\n",
    "    def LogTextPathsRetriever(self):\n",
    "        \n",
    "        textlogpaths = []\n",
    "        chronological_dates = []\n",
    "        nonreal_dates = []\n",
    "        chronological_textlogpaths = []\n",
    "        \n",
    "        for yearfold in self.year_folders:\n",
    "            iterloglist = ListDirNoDSstore(os.path.join(\n",
    "                self.path_to_logs,\n",
    "                yearfold))\n",
    "            for textfilename in iterloglist:\n",
    "                textlogpaths.append(os.path.join(\n",
    "                    self.path_to_logs,\n",
    "                    yearfold,\n",
    "                    textfilename))\n",
    "        \n",
    "        for path in textlogpaths:\n",
    "            firstnamesplit = path.split('/Logs/')[1]\n",
    "            secondnamesplit = firstnamesplit.split('/Log')[1]\n",
    "            thirdnamesplit = secondnamesplit.split('.txt')[0]\n",
    "            try:\n",
    "                date_time_obj = datetime.fromisoformat(thirdnamesplit)\n",
    "            except:\n",
    "                nonreal_dates.append(thirdnamesplit)\n",
    "            else:\n",
    "                chronological_dates.append(date_time_obj)\n",
    "            \n",
    "        chronological_dates.sort()\n",
    "        \n",
    "        for baddate in nonreal_dates:\n",
    "            splitbaddate = baddate.split('-')\n",
    "            pathtobaddatetxt = os.path.join(self.path_to_logs,\n",
    "                                           splitbaddate[0],\n",
    "                                           f'Log{baddate}.txt')\n",
    "            chronological_textlogpaths.append(pathtobaddatetxt)\n",
    "        \n",
    "        for timeobj in chronological_dates:\n",
    "            itr_year = int(timeobj.year)\n",
    "            itr_month = int(timeobj.month)\n",
    "            itr_day = int(timeobj.day)\n",
    "            if itr_month < 10:\n",
    "                itr_month = '0'+str(itr_month)\n",
    "            if itr_day < 10:\n",
    "                itr_day = '0'+str(itr_day)\n",
    "            newiterpath = os.path.join(self.path_to_logs,\n",
    "                                      str(itr_year),\n",
    "                                      f'Log{itr_year}-{itr_month}-{itr_day}.txt')\n",
    "            chronological_textlogpaths.append(newiterpath)\n",
    "        \n",
    "        return chronological_textlogpaths\n",
    "    \n",
    "    \n",
    "    def Log_CSV(self):\n",
    "        \n",
    "        firstdayincycle = 'null'\n",
    "        lastdayincycle = 'null'\n",
    "        \n",
    "        CurrentCSVtoAppendto = 'null'\n",
    "        CurrentColNames = 'null'\n",
    "        CurrentHeaderLength = 'null'\n",
    "        \n",
    "        if not self.LogTextPathsRetriever():\n",
    "            print(f\"No .txt logs in {self.subfoldername} of {self.nodename}.\")\n",
    "            pass\n",
    "        else:\n",
    "            if LogInfoGetter((self.LogTextPathsRetriever())[0]) == 'error: no data in .txt':\n",
    "                pass\n",
    "            else:\n",
    "                firstloginfo = LogInfoGetter((self.LogTextPathsRetriever())[0])\n",
    "                firstdayincycle = firstloginfo['LogDate']\n",
    "                CurrentCSVtoAppendto = os.path.join(self.path_to_logs,\n",
    "                                               f'Logs_{firstdayincycle}.csv')\n",
    "                CurrentColNames = firstloginfo['ColNames']\n",
    "                CurrentHeaderLength  = len(CurrentColNames)\n",
    "        \n",
    "                with open(CurrentCSVtoAppendto,'w',newline='') as csvfile:\n",
    "                    fieldnames = CurrentColNames\n",
    "                    thewriter = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                    thewriter.writeheader()\n",
    "                    for D in range(len(firstloginfo['Data'])):\n",
    "                        iter_dict = {}\n",
    "                        for i in range(len((firstloginfo)['ColNames'])):\n",
    "                            iter_dict[firstloginfo['ColNames'][i]] = firstloginfo['Data'][D][i]\n",
    "                        thewriter.writerow(iter_dict)\n",
    "                    print(f\"<<{self.nodename}, {self.subfoldername}>> Created ~Logs_{firstdayincycle}.csv~ at base of /Logs/\")\n",
    "                    csvfile.close()\n",
    "                \n",
    "            textfile_iteration_counter = 0\n",
    "            while textfile_iteration_counter < len(self.LogTextPathsRetriever())-1: \n",
    "                for i in range(len(self.LogTextPathsRetriever())):\n",
    "                    if i != 0:\n",
    "                        iterloginfo = LogInfoGetter((self.LogTextPathsRetriever())[i])\n",
    "                        if iterloginfo  == 'error: no data in .txt':\n",
    "                            textfile_iteration_counter+=1\n",
    "                            pass\n",
    "                        else:\n",
    "                            iterlogColNames = iterloginfo['ColNames']\n",
    "                            #iterlogHeaderLength = len(iterlogColNames)\n",
    "                \n",
    "                            if iterlogColNames == CurrentColNames:\n",
    "                                with open(CurrentCSVtoAppendto,'a',newline='') as csvfile:\n",
    "                                    dictwriter_object = DictWriter(csvfile,\n",
    "                                                               fieldnames=CurrentColNames)\n",
    "                                    for D in range(len(iterloginfo['Data'])):\n",
    "                                        iter_dict = {}\n",
    "                                        for i in range(len((iterloginfo)['ColNames'])):\n",
    "                                            iter_dict[iterloginfo['ColNames'][i]] = iterloginfo['Data'][D][i]\n",
    "                                        dictwriter_object.writerow(iter_dict)\n",
    "                                    csvfile.close()\n",
    "                                lastdayincycle = iterloginfo['LogDate']\n",
    "                                newCSVname = os.path.join(\n",
    "                                    self.path_to_logs,\n",
    "                                    f'Logs_{firstdayincycle}_to_{lastdayincycle}.csv'\n",
    "                                )\n",
    "                                os.rename(CurrentCSVtoAppendto,\n",
    "                                         newCSVname)\n",
    "                                CurrentCSVtoAppendto = newCSVname\n",
    "                                textfile_iteration_counter += 1\n",
    "                                if textfile_iteration_counter > len(self.LogTextPathsRetriever()):\n",
    "                                    break\n",
    "                            \n",
    "                            else:\n",
    "                                pastCSV_date_presplit = CurrentCSVtoAppendto.split('.csv')[0]\n",
    "                                pastCSV_date = pastCSV_date_presplit.split('_')[-1]\n",
    "                                CurrentCSVtoAppendto = os.path.join(self.path_to_logs,\n",
    "                                                                    'Logs_'+iterloginfo[\"LogDate\"]+'.csv')\n",
    "                                CurrentColNames = iterlogColNames\n",
    "                                firstdayincycle = iterloginfo['LogDate']\n",
    "                                lastdayincycle = 'null'\n",
    "                                with open(CurrentCSVtoAppendto,'w',newline='') as csvfile:\n",
    "                                    fieldnames = CurrentColNames\n",
    "                                    thewriter = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                                    thewriter.writeheader()\n",
    "                                    for D in range(len(iterloginfo['Data'])):\n",
    "                                        iter_dict = {}\n",
    "                                        for i in range(len((iterloginfo)['ColNames'])):\n",
    "                                            iter_dict[iterloginfo['ColNames'][i]] = iterloginfo['Data'][D][i]\n",
    "                                        thewriter.writerow(iter_dict)\n",
    "                                    print(f\"<<{self.nodename}, {self.subfoldername}>> Inconsistent log field names/amount between {pastCSV_date} and {firstdayincycle};\"+\n",
    "                                          f\" Starting new CSV file (which following logs will be appended to) at base of /Logs/\")\n",
    "                                    csvfile.close()\n",
    "                                textfile_iteration_counter += 1\n",
    "                                if textfile_iteration_counter > len(self.LogTextPathsRetriever()):\n",
    "                                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running functions on every SD card folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<Winter_Turf_Type_B_-_6, e00fce685b02f35fe13a0a2d>> Created ~Logs_2000-00-01.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_1, e00fce6838cebdf42fc24391>> Created ~Logs_2000-00-01.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_1, e00fce6838cebdf42fc24391>> Inconsistent log field names/amount between 2000-00-01 and 2021-11-30.1; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_1, e00fce6838cebdf42fc24391>> Inconsistent log field names/amount between 2021-11-30.1 and 2020-11-17; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_7, e00fce682a79a64999b7b409>> Created ~Logs_2000-00-01.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_7, e00fce682a79a64999b7b409>> Inconsistent log field names/amount between 2021-10-28 and 2021-12-13; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_8, e00fce6829163099f836ae78>> Inconsistent log field names/amount between null and 2000-00-00; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_8, e00fce6829163099f836ae78>> Inconsistent log field names/amount between 2000-00-00 and 2001-01-01.1; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_8, e00fce6829163099f836ae78>> Inconsistent log field names/amount between 2001-01-01.1 and 2165-25-45; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_8, e00fce6829163099f836ae78>> Inconsistent log field names/amount between 2020-11-14 and 2021-12-10; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_10, e00fce68206506b1159c8936>> Created ~Logs_2000-00-01.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_10, e00fce68206506b1159c8936>> Inconsistent log field names/amount between 2020-12-28 and 2021-12-10; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_28, e00fce683d1ce7e541f9698f>> Created ~Logs_2021-12-10.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_17, e00fce686565dea3e22b623c>> Created ~Logs_2021-12-10.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_21, e00fce68af529fe2d2d7c809>> Inconsistent log field names/amount between null and 2000-01-01; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_19, e00fce682d4d9ca0f9a3b12d>> Created ~Logs_2021-12-10.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_26, e00fce684b4112eb5390b0a0>> Created ~Logs_2021-12-10.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_18, e00fce68e485873e6b6e983b>> Created ~Logs_2021-12-10.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_27, e00fce68e5bd8b129dc5e774>> Created ~Logs_2021-12-10.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_20, e00fce6867d3a0cda48e32a2>> Inconsistent log field names/amount between null and 2021-12-10; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_16, e00fce682699a15d165801b1>> Created ~Logs_2001-01-01.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_16, e00fce682699a15d165801b1>> Inconsistent log field names/amount between 2021-03-18 and 2021-12-08; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_16, e00fce682699a15d165801b1>> Inconsistent log field names/amount between 2021-12-09 and 2021-12-13; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_34, e00fce687038a71d446ef776>> Inconsistent log field names/amount between null and 2025-03-14; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "No .txt logs in e00fce68c2bc8e9d3b033655 of Winter_Turf_Type_A_-_34.\n",
      "<<Winter_Turf_Type_B_-_12, e00fce6891afbb6bddd89915>> Created ~Logs_2000-00-00.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_12, e00fce6891afbb6bddd89915>> Inconsistent log field names/amount between 2000-00-00 and 2001-01-01; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_12, e00fce6891afbb6bddd89915>> Inconsistent log field names/amount between 2021-11-10 and 2021-11-11; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_35, e00fce6857f0346b44cd699d>> Created ~Logs_2021-12-10.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_5, e00fce6856706b033b691f8b>> Created ~Logs_2000-00-01.1.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_5, e00fce6856706b033b691f8b>> Inconsistent log field names/amount between 2000-00-01.1 and 2000-00-01; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_3, e00fce6844eb3e18a15dc07f>> Created ~Logs_2000-00-00.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_3, e00fce6844eb3e18a15dc07f>> Inconsistent log field names/amount between 2021-10-27 and 2021-12-10; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_2, e00fce68816c2bc59976cdf2>> Created ~Logs_2165-25-45.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_2, e00fce68816c2bc59976cdf2>> Inconsistent log field names/amount between 2020-11-20 and 2021-12-10; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_5, e00fce68c014249653ebc049>> Created ~Logs_2001-01-01.1.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_5, e00fce68c014249653ebc049>> Inconsistent log field names/amount between 2001-01-01.1 and 2071-00-24; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_5, e00fce68c014249653ebc049>> Inconsistent log field names/amount between 2001-02-22 and 2021-12-10; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_4, e00fce682c26b88b84ab26f2>> Created ~Logs_2001-01-01.1.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_4, e00fce682c26b88b84ab26f2>> Inconsistent log field names/amount between 2021-11-02.1 and 2001-01-01; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_4, e00fce682c26b88b84ab26f2>> Inconsistent log field names/amount between 2001-01-01 and 2001-01-02; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_4, e00fce682c26b88b84ab26f2>> Inconsistent log field names/amount between 2001-01-02 and 2020-11-20; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_4, e00fce682c26b88b84ab26f2>> Inconsistent log field names/amount between 2021-04-04 and 2021-12-01; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_14, e00fce68428e95de642bb0d9>> Created ~Logs_2001-01-01.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_14, e00fce68428e95de642bb0d9>> Inconsistent log field names/amount between 2021-10-28 and 2021-12-10; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_22, e00fce68d387c98c80fc79bc>> Inconsistent log field names/amount between null and 2021-12-10; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_23, e00fce68f5490112d61bdccb>> Created ~Logs_2000-01-30.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_24, e00fce68a0322349e64ec178>> Created ~Logs_2022-01-16.1.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_24, e00fce68a0322349e64ec178>> Inconsistent log field names/amount between 2022-01-16.1 and 2032-02-01; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_24, e00fce681171349b5773ef72>> Inconsistent log field names/amount between null and 2021-12-03.1; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_24, e00fce681171349b5773ef72>> Inconsistent log field names/amount between 2021-12-03.1 and 2021-12-11; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<Winter_Turf_Type_A_-_12, e00fce68b6dc19eefd628b4e>> Created ~Logs_2000-00-21.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_12, e00fce68b6dc19eefd628b4e>> Inconsistent log field names/amount between 2000-00-01 and 2000-00-00; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_12, e00fce68b6dc19eefd628b4e>> Inconsistent log field names/amount between 2165-25-45 and 2020-11-14; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_12, e00fce68b6dc19eefd628b4e>> Inconsistent log field names/amount between 2020-12-25 and 2021-12-11; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_15, e00fce68ccb7400e1fb8aa98>> Created ~Logs_2000-00-01.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_15, e00fce68ccb7400e1fb8aa98>> Inconsistent log field names/amount between 2021-03-24 and 2021-12-10; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_11, e00fce68fb8a948ec07bb826>> Created ~Logs_2000-00-01.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_11, e00fce68fb8a948ec07bb826>> Inconsistent log field names/amount between 2021-11-00 and 2021-11-16.1; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_B_-_11, e00fce68fb8a948ec07bb826>> Inconsistent log field names/amount between 2021-11-16.1 and 2020-11-17; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_31, e00fce6813794773754ac5b9>> Created ~Logs_2002-07-24.csv~ at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_31, e00fce688c30491f7bf87fb2>> Inconsistent log field names/amount between null and 2021-12-14.1; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_31, e00fce688c30491f7bf87fb2>> Inconsistent log field names/amount between 2021-12-14.1 and 2021-12-01; Starting new CSV file (which following logs will be appended to) at base of /Logs/\n",
      "<<Winter_Turf_Type_A_-_36, e00fce6848a6a48956efa89d>> Created ~Logs_2021-12-10.csv~ at base of /Logs/\n"
     ]
    }
   ],
   "source": [
    "for nodefld in node_folders:\n",
    "    iternodefld = NodeFolder(nodefld)\n",
    "    for subfldr in iternodefld.node_subfolders:\n",
    "        itersubfld = SubNodeFolder(subfldr,\n",
    "                                   iternodefld.nodename,\n",
    "                                   iternodefld.gems_folderpath)\n",
    "        itersubfld.Log_CSV() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
